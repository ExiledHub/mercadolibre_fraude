{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize label encoder for categorical data encoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Read the preprocessed and balanced data for model training\n",
    "data = pd.read_csv('./data/silver/data.csv')\n",
    "y_train = data['fraude']  # Extract the target variable 'fraude'\n",
    "X_train = data.drop('fraude', axis=1)  # Drop the target variable to leave only features\n",
    "\n",
    "# Read the raw data for testing and validation\n",
    "data_raw = pd.read_csv('./data/bronze/dataset.csv')\n",
    "y_raw = data_raw['fraude']  # Extract the target variable 'fraude' from the raw data\n",
    "X_raw = data_raw.drop('fraude', axis=1)  # Drop the target variable to leave only features\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = ['a', 'd', 'score', 'fraude', 'g', 'n', 'o', 'p']\n",
    "\n",
    "# Identify and define continuous numerical columns\n",
    "continuous_numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_columns)\n",
    "\n",
    "# Convert categorical columns to category type and encode them\n",
    "for column in [col for col in categorical_columns if col not in ['fraude']]:\n",
    "    X_raw[column] = X_raw[column].astype('category').cat.codes\n",
    "\n",
    "# Generate date-related features for the raw data as was done for the training data\n",
    "# Convert 'fecha' column to datetime\n",
    "X_raw['fecha'] = pd.to_datetime(X_raw['fecha'])\n",
    "\n",
    "# Feature engineering\n",
    "# Feature 1: Day of the week\n",
    "X_raw['day_of_week'] = X_raw['fecha'].dt.dayofweek\n",
    "categorical_columns.append('day_of_week')\n",
    "\n",
    "# Feature 2: Part of the day, encoded as 1 (00-06h), 2 (06-12h), 3 (12-18h), 4 (18-24h)\n",
    "X_raw['part_of_day'] = pd.cut(X_raw['fecha'].dt.hour, \n",
    "                              bins=[0, 6, 12, 18, 24], \n",
    "                              include_lowest=True, \n",
    "                              labels=[1, 2, 3, 4])\n",
    "categorical_columns.append('part_of_day')\n",
    "\n",
    "# Feature 3: Week of the year\n",
    "X_raw['week_of_year'] = X_raw['fecha'].dt.isocalendar().week\n",
    "categorical_columns.append('week_of_year')\n",
    "\n",
    "# Feature 4: Is it a weekend? (1 for Saturday/Sunday, 0 otherwise)\n",
    "X_raw['is_weekend'] = (X_raw['fecha'].dt.dayofweek >= 5).astype(int)\n",
    "categorical_columns.append('is_weekend')\n",
    "\n",
    "# Drop the original 'fecha' column if it's no longer needed\n",
    "X_raw.drop('fecha', axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values in numerical columns with the mean of each column\n",
    "for column in continuous_numerical_columns:\n",
    "    try:\n",
    "        X_raw[column] = X_raw[column].fillna(X_raw[column].mean())\n",
    "    except:\n",
    "        pass  # If an error occurs, pass to the next column\n",
    "\n",
    "# Split raw data into testing and validation sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_raw, y_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure that all numerical data is of float type for consistency\n",
    "for col in X_train.columns:\n",
    "    if pd.api.types.is_integer_dtype(X_train[col]) or pd.api.types.is_float_dtype(X_train[col]):\n",
    "        X_train[col] = X_train[col].astype(float)\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if pd.api.types.is_integer_dtype(X_test[col]) or pd.api.types.is_float_dtype(X_test[col]):\n",
    "        X_test[col] = X_test[col].astype(float)\n",
    "\n",
    "# Define numerical and categorical columns after type conversion\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),  # Standardize numerical features\n",
    "        ('cat', OneHotEncoder(), categorical_cols)  # One-hot encode categorical features\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the training data and transform both the training and testing data\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "y_train_encoded = y_train\n",
    "y_test_encoded = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_encoded = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the logistic regression model\n",
    "pickle_file_logit = './models/logreg_model.pkl'\n",
    "\n",
    "# Check if the logistic regression model exists; if so, load it; if not, train and save it\n",
    "if os.path.isfile(pickle_file_logit):\n",
    "    with open(pickle_file_logit, 'rb') as file:\n",
    "        best_logit_model = pickle.load(file)\n",
    "else:\n",
    "    # Define grid search parameters for logistic regression\n",
    "    param_grid_logit = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    }\n",
    "    \n",
    "    # Initialize grid search for logistic regression\n",
    "    grid_search_logit = GridSearchCV(\n",
    "        estimator=LogisticRegression(max_iter=1000),\n",
    "        param_grid=param_grid_logit,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    # Execute grid search\n",
    "    grid_search_logit.fit(X_train_encoded, y_train_encoded)\n",
    "    \n",
    "    # Save the best logistic regression model\n",
    "    best_logit_model = grid_search_logit.best_estimator_\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    with open(pickle_file_logit, 'wb') as file:\n",
    "        pickle.dump(best_logit_model, file)\n",
    "\n",
    "# Make predictions and evaluate the logistic regression model\n",
    "y_pred_logit = best_logit_model.predict(X_test_encoded)\n",
    "model_results_encoded['Logistic Regression'] = {\n",
    "    'Classification Report': classification_report(y_test_encoded, y_pred_logit),\n",
    "    'ROC AUC Score': roc_auc_score(y_test_encoded, best_logit_model.predict_proba(X_test_encoded)[:, 1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the random forest model\n",
    "pickle_file_rf = './models/random_forest_model.pkl'\n",
    "\n",
    "# Check if the random forest model exists; if so, load it; if not, train and save it\n",
    "if os.path.isfile(pickle_file_rf):\n",
    "    with open(pickle_file_rf, 'rb') as file:\n",
    "        best_rf_model = pickle.load(file)\n",
    "else:\n",
    "    # Define grid search parameters for random forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # Initialize grid search for random forest\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid_rf,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    # Execute grid search\n",
    "    grid_search_rf.fit(X_train_encoded, y_train_encoded)\n",
    "    \n",
    "    # Save the best random forest model\n",
    "    best_rf_model = grid_search_rf.best_estimator_\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    with open(pickle_file_rf, 'wb') as file:\n",
    "        pickle.dump(best_rf_model, file)\n",
    "\n",
    "# Make predictions and evaluate the random forest model\n",
    "y_pred_rf = best_rf_model.predict(X_test_encoded)\n",
    "model_results_encoded['Random Forest'] = {\n",
    "    'Classification Report': classification_report(y_test_encoded, y_pred_rf),\n",
    "    'ROC AUC Score': roc_auc_score(y_test_encoded, best_rf_model.predict_proba(X_test_encoded)[:, 1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the SVM model\n",
    "pickle_file_svm = './models/svm_model.pkl'\n",
    "\n",
    "# Check if the SVM model exists; if so, load it; if not, train and save it\n",
    "if os.path.isfile(pickle_file_svm):\n",
    "    with open(pickle_file_svm, 'rb') as file:\n",
    "        svm_model = pickle.load(file)\n",
    "else:\n",
    "    # Initialize the SVM with specified hyperparameters\n",
    "    svm_model = SVC(\n",
    "        kernel='linear',\n",
    "        C=0.1,\n",
    "        cache_size=700,\n",
    "        tol=1e-3,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the SVM\n",
    "    svm_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "    # Save the SVM model to a pickle file\n",
    "    with open(pickle_file_svm, 'wb') as file:\n",
    "        pickle.dump(svm_model, file)\n",
    "\n",
    "# Make predictions and evaluate the SVM model\n",
    "y_pred_svm = svm_model.predict(X_test_encoded)\n",
    "model_results_encoded['Support Vector Machine'] = {\n",
    "    'Classification Report': classification_report(y_test_encoded, y_pred_svm),\n",
    "    'ROC AUC Score': roc_auc_score(y_test_encoded, svm_model.predict_proba(X_test_encoded)[:, 1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the ANN model\n",
    "pickle_file_ann = './models/ann_model.pkl'\n",
    "\n",
    "# Check if the ANN model exists; if so, load it; if not, train and save it\n",
    "if os.path.isfile(pickle_file_ann):\n",
    "    with open(pickle_file_ann, 'rb') as file:\n",
    "        ann_model = pickle.load(file)\n",
    "else:\n",
    "    # Initialize the ANN with specified hyperparameters\n",
    "    ann_model = MLPClassifier(alpha=0.01, random_state=42, max_iter=1000)\n",
    "\n",
    "    # Train the ANN\n",
    "    ann_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "    # Save the ANN model to a pickle file\n",
    "    with open(pickle_file_ann, 'wb') as file:\n",
    "        pickle.dump(ann_model, file)\n",
    "\n",
    "# Make predictions and evaluate the ANN model\n",
    "y_pred_ann = ann_model.predict(X_test_encoded)\n",
    "model_results_encoded['Artificial Neural Network'] = {\n",
    "    'Classification Report': classification_report(y_test_encoded, y_pred_ann),\n",
    "    'ROC AUC Score': roc_auc_score(y_test_encoded, ann_model.predict_proba(X_test_encoded)[:, 1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Info] Number of positive: 76786, number of negative: 76786\n",
      "[LightGBM] [Info] Total Bins 1800\n",
      "[LightGBM] [Info] Number of data points in the train set: 153572, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n"
     ]
    }
   ],
   "source": [
    "# File path for the LightGBM model\n",
    "pickle_file_lgbm = './models/lgbm_model.pkl'\n",
    "max_depth = 7\n",
    "\n",
    "# Define grid search parameters for LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_data_in_leaf': [10, 15, 20],\n",
    "    'num_leaves': [2**i - 1 for i in range(max_depth, max_depth+1)],\n",
    "    'reg_lambda': [0.01, 0.1, 1],\n",
    "    'max_depth': [6, 7, 8],\n",
    "}\n",
    "\n",
    "# Check if the LightGBM model exists; if so, load it; if not, train and save it\n",
    "if os.path.isfile(pickle_file_lgbm):\n",
    "    with open(pickle_file_lgbm, 'rb') as file:\n",
    "        best_lgbm_model = pickle.load(file)\n",
    "else:\n",
    "    # Initialize LightGBM with forced row-wise computation\n",
    "    lgbm_classifier = LGBMClassifier(force_row_wise=True)\n",
    "\n",
    "    # Initialize grid search for LightGBM\n",
    "    grid_search_lgbm = GridSearchCV(\n",
    "        estimator=lgbm_classifier,\n",
    "        param_grid=param_grid_lgbm,\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Execute grid search\n",
    "    grid_search_lgbm.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "    # Save the best LightGBM model\n",
    "    best_lgbm_model = grid_search_lgbm.best_estimator_\n",
    "\n",
    "    # Save the model to a pickle file\n",
    "    with open(pickle_file_lgbm, 'wb') as file:\n",
    "        pickle.dump(best_lgbm_model, file)\n",
    "\n",
    "# Make predictions and evaluate the LightGBM model\n",
    "y_pred_lgbm = best_lgbm_model.predict(X_test_encoded)\n",
    "model_results_encoded['LightGBM'] = {\n",
    "    'Classification Report': classification_report(y_test_encoded, y_pred_lgbm),\n",
    "    'ROC AUC Score': roc_auc_score(y_test_encoded, best_lgbm_model.predict_proba(X_test_encoded)[:, 1])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Logistic Regression\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.76      0.86    114024\n",
      "           1       0.13      0.68      0.22      5976\n",
      "\n",
      "    accuracy                           0.76    120000\n",
      "   macro avg       0.55      0.72      0.54    120000\n",
      "weighted avg       0.94      0.76      0.82    120000\n",
      "\n",
      "ROC AUC Score: 0.7848785662775519\n",
      "\n",
      "Modelo: Random Forest\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95    114024\n",
      "           1       0.30      0.60      0.40      5976\n",
      "\n",
      "    accuracy                           0.91    120000\n",
      "   macro avg       0.64      0.76      0.68    120000\n",
      "weighted avg       0.94      0.91      0.92    120000\n",
      "\n",
      "ROC AUC Score: 0.8705662018146723\n",
      "\n",
      "Modelo: Support Vector Machine\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86    114024\n",
      "           1       0.13      0.67      0.22      5976\n",
      "\n",
      "    accuracy                           0.76    120000\n",
      "   macro avg       0.55      0.72      0.54    120000\n",
      "weighted avg       0.94      0.76      0.83    120000\n",
      "\n",
      "ROC AUC Score: 0.7874094808805605\n",
      "\n",
      "Modelo: Artificial Neural Network\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    114024\n",
      "           1       0.43      0.10      0.16      5976\n",
      "\n",
      "    accuracy                           0.95    120000\n",
      "   macro avg       0.69      0.54      0.57    120000\n",
      "weighted avg       0.93      0.95      0.93    120000\n",
      "\n",
      "ROC AUC Score: 0.7080122992613593\n",
      "\n",
      "Modelo: LightGBM\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92    114024\n",
      "           1       0.18      0.57      0.28      5976\n",
      "\n",
      "    accuracy                           0.85    120000\n",
      "   macro avg       0.58      0.72      0.60    120000\n",
      "weighted avg       0.94      0.85      0.89    120000\n",
      "\n",
      "ROC AUC Score: 0.8074434246257933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification reports and ROC AUC scores for each model\n",
    "for model_name, results in model_results_encoded.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(results['Classification Report'])\n",
    "    print(f\"ROC AUC Score: {results['ROC AUC Score']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "La precision y el recall se vuelve un problema en nuestro caso, con algunos modelos con buen recall pero mala precision y algunos con una mejor precision pero un recall terrible. Teniendo esto en cuenta, podriamos llegar a mejorar los resultados en general con ayuda de un ensemble model, que nos permita tener varios modelos funcionando a la vez y tomando decisiones sobre estos. Esto nos podria ayudar a suabisar la diferencia entre la precision y el recall que tenemos en este momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "Accuracy: 0.9036\n",
      "Precision: 0.2854\n",
      "Recall: 0.6223\n",
      "F1-score: 0.3914\n",
      "ROC AUC Score: 0.8728\n"
     ]
    }
   ],
   "source": [
    "# Define a list of (name, model) tuples for the models to be included in the ensemble\n",
    "estimators = [\n",
    "    ('logistic', best_logit_model),\n",
    "    ('random_forest', best_rf_model),\n",
    "    ('lightgbm', best_lgbm_model)\n",
    "]\n",
    "\n",
    "# Create the ensemble model using a voting classifier with 'soft' voting\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Fit the ensemble model using the training data\n",
    "ensemble.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Make predictions with the ensemble model on the test data\n",
    "ensemble_predictions = ensemble.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the ensemble model's accuracy on the test data\n",
    "ensemble_score = ensemble.score(X_test_encoded, y_test_encoded)\n",
    "\n",
    "# Save the trained ensemble model to a pickle file for later use or deployment\n",
    "with open('./models/ensemble_model_pres.pkl', 'wb') as file:\n",
    "    pickle.dump(ensemble, file)\n",
    "\n",
    "# Calculate various performance metrics to evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test_encoded, ensemble_predictions)\n",
    "precision = precision_score(y_test_encoded, ensemble_predictions)\n",
    "recall = recall_score(y_test_encoded, ensemble_predictions)\n",
    "f1 = f1_score(y_test_encoded, ensemble_predictions)\n",
    "roc_auc = roc_auc_score(y_test_encoded, ensemble.predict_proba(X_test_encoded)[:, 1])\n",
    "\n",
    "# Print out the performance metrics for the ensemble model\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Final\n",
    "Teniendo en cuenta la relacion entre la precision y el recall, nuestro modelo esta generando muchos falsos positivos, siendo el 80% de las predicciones, falsos positivos. Aun asi, el recall nos dice que en efecto de todas las transacciones marcadas como fraudulentas en el dataset, el 67% las logro identificar. Esto podria mejorarse despues de hacer un proceso de fine tunening adecuado, y apesar de tener un ROC AUC de 85%, el modelo todavia tiene mucho margen de mejora, en especial con el recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "Accuracy: 0.9421\n",
      "Precision: 0.4182\n",
      "Recall: 0.4145\n",
      "F1-score: 0.4163\n",
      "ROC AUC Score: 0.8730\n"
     ]
    }
   ],
   "source": [
    "# Define a list of (name, model) tuples including the models for the ensemble: logistic regression, random forest, LightGBM, and ANN\n",
    "estimators = [\n",
    "    ('logistic', best_logit_model),\n",
    "    ('random_forest', best_rf_model),\n",
    "    ('lightgbm', best_lgbm_model),\n",
    "    ('ann', ann_model)\n",
    "]\n",
    "\n",
    "# Create the ensemble model with the voting classifier using 'soft' voting\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Fit the ensemble model to the training data\n",
    "ensemble.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Make predictions using the ensemble model on the test dataset\n",
    "ensemble_predictions = ensemble.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the accuracy of the ensemble model on the test data\n",
    "ensemble_score = ensemble.score(X_test_encoded, y_test_encoded)\n",
    "\n",
    "# Save the trained ensemble model to a pickle file for future use\n",
    "with open('./models/ensemble_model_bal.pkl', 'wb') as file:\n",
    "    pickle.dump(ensemble, file)\n",
    "\n",
    "# Calculate several performance metrics for the ensemble model\n",
    "accuracy = accuracy_score(y_test_encoded, ensemble_predictions)  # Overall accuracy\n",
    "precision = precision_score(y_test_encoded, ensemble_predictions)  # Precision of positive predictions\n",
    "recall = recall_score(y_test_encoded, ensemble_predictions)  # Recall (true positive rate)\n",
    "f1 = f1_score(y_test_encoded, ensemble_predictions)  # Harmonic mean of precision and recall\n",
    "roc_auc = roc_auc_score(y_test_encoded, ensemble.predict_proba(X_test_encoded)[:, 1])  # ROC-AUC score using probability estimates for the positive class\n",
    "\n",
    "# Print out the calculated performance metrics for the ensemble model\n",
    "print(f'Accuracy: {accuracy:.4f}')   # Print accuracy rounded to 4 decimal places\n",
    "print(f'Precision: {precision:.4f}')  # Print precision rounded to 4 decimal places\n",
    "print(f'Recall: {recall:.4f}')       # Print recall rounded to 4 decimal places\n",
    "print(f'F1-score: {f1:.4f}')          # Print F1-score rounded to 4 decimal places\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}') # Print ROC AUC score rounded to 4 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
